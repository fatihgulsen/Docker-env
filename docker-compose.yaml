version: "3.7"

######################################################
# COMMONS
######################################################

x-sparkworker-common: &sparkworker-common
  restart: on-failure
  build:
    context: .
    dockerfile: spark/Dockerfile
  env_file:
    - ./shared/configs/common.env
    - ./spark/config/spark.env
  environment:
    - SPARK_MODE=worker
  volumes:
    - ./logs/spark/work/:/opt/bitnami/spark/work/
    - ./logs/spark/logs/:/opt/bitnami/spark/history/
    - ./airflow/dags:/opt/bitnami/airflow/dags
    - ./shared/scripts/:/opt/bitnami/spark/scripts
    - ./spark/config/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
    - ./spark/config/log4j.properties:/opt/bitnami/spark/conf/log4j.properties
    - ./hadoop/config:/opt/hadoop/etc/hadoop
  networks:
    - etl-net

x-airflow-common: &airflow-common
  # restart: on-failure
  env_file:
    - ./shared/configs/common.env
    - ./airflow/config/airflow.env
  networks:
    - etl-net
  volumes:
    - ./airflow/dags:/opt/bitnami/airflow/dags
    - ./logs/airflow/logs:/opt/bitnami/airflow/logs
    - ./airflow/plugins:/opt/bitnami/airflow/plugins
    - ./airflow/requirements.txt:/bitnami/python/requirements.txt
  depends_on:
    - minio
    - postgres
    - redis

services:
  ######################################################
  # DATABASE SERVICE
  ######################################################

  postgres:
    image: postgres:16
    container_name: postgres
    restart: always
    ports:
      - 5401:5432
    networks:
      etl-net:
        ipv4_address: 10.5.0.101
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./shared/scripts/init_postgres_database.sql:/docker-entrypoint-initdb.d/init_postgresql_database.sql
    logging:
      driver: "json-file"
      options:
        max-file: "5"
        max-size: "10m"
    healthcheck:
      test: [ "CMD", "pg_isready", "-q", "-d", "postgres", "-U", "postgres" ]
      timeout: 45s
      interval: 10s
      retries: 10

  redis:
    image: redis:7.2-alpine
    container_name: redis
    restart: on-failure
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    environment:
      - ALLOW_EMPTY_PASSWORD=yes
    networks:
      etl-net:
        ipv4_address: 10.5.0.102

  ######################################################
  # MINIO SERVICE
  ######################################################

  minio:
    image: docker.io/bitnami/minio:2023
    container_name: minio
    restart: on-failure
    environment:
      - MINIO_ROOT_USER=minio
      - MINIO_ROOT_PASSWORD=miniosecret
    healthcheck:
      test:
        [
          "CMD",
          "curl",
          "-f",
          "http://localhost:9000/minio/health/live"
        ]
      interval: 30s
      timeout: 20s
      retries: 3
    ports:
      - 9000:9000
      - 9001:9001
    volumes:
      - minio_data:/bitnami/minio/data
      - minio_certs:/certs
    networks:
      etl-net:
        ipv4_address: 10.5.0.103

  minio-client:
    image: docker.io/bitnami/minio-client:2023
    user: root
    restart: on-failure
    container_name: minio-client
    depends_on:
      - minio
    networks:
      etl-net:
    environment:
      - MINIO_SERVER_HOST=minio
      - MINIO_SERVER_ACCESS_KEY=minio
      - MINIO_SERVER_SECRET_KEY=miniosecret
    entrypoint: >
      /bin/sh -c " mc alias set minio http://minio:9000 minio miniosecret; mc mb minio/spark-logs; mc mb minio/tmp; mc mb minio/airflow-logs; mc mb minio/druid-logs; mc admin user add minio airflow airflow_secret; echo 'Added user airflow.'; mc admin policy set minio readwrite user=airflow; exit 0; "

  ######################################################
  # AIRFLOW SERVICES
  ######################################################

  airflow-scheduler:
    build:
      context: .
      dockerfile: airflow/scheduler/Dockerfile
    container_name: airflow-scheduler
    <<: *airflow-common

  airflow-worker:
    build:
      context: .
      dockerfile: airflow/worker/Dockerfile
    container_name: airflow-worker
    <<: *airflow-common

  airflow:
    build:
      context: .
      dockerfile: airflow/master/Dockerfile
    container_name: airflow
    ports:
      - 8080:8080
    <<: *airflow-common

  ######################################################
  # SPARK SERVICE
  ######################################################

  spark-master:
    build:
      context: .
      dockerfile: spark/Dockerfile
    container_name: spark-master
    restart: on-failure
    env_file:
      - ./shared/configs/common.env
      - ./spark/config/spark.env
    environment:
      - SPARK_MODE=master
    ports:
      - '4040:8080'
    volumes:
      - ./logs/spark/work/:/opt/bitnami/spark/work/
      - ./logs/spark/logs/:/opt/bitnami/spark/history/
      - ./shared/scripts/:/opt/bitnami/spark/scripts
      - ./airflow/dags:/opt/bitnami/airflow/dags
      - ./spark/config/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
      - ./spark/config/log4j.properties:/opt/bitnami/spark/conf/log4j.properties
      - ./hadoop/config:/opt/hadoop/etc/hadoop
    networks:
      - etl-net

  spark-worker-1:
    <<: *sparkworker-common
    container_name: spark-worker-1

  spark-worker-2:
    <<: *sparkworker-common
    container_name: spark-worker-2
  
  ######################################################
  # DRUID SERVICE
  ######################################################

  zookeeper:
    container_name: zookeeper
    image: zookeeper:3.5.10
    ports:
      - "2181:2181"
    environment:
      - ZOO_MY_ID=1
    networks:
      - etl-net

  druid-coordinator:
    image: apache/druid:30.0.0
    container_name: druid-coordinator
    volumes:
      - druid_shared:/opt/shared
      - coordinator_var:/opt/druid/var
    depends_on:
      - zookeeper
      - postgres
    ports:
      - "8081:8081"
    command:
      - coordinator
    env_file:
      - ./druid/config/druid.env
    networks:
      - etl-net

  druid-broker:
    image: apache/druid:30.0.0
    container_name: druid-broker
    volumes:
      - broker_var:/opt/druid/var
    depends_on:
      - zookeeper
      - postgres
      - druid-coordinator
    ports:
      - "8082:8082"
    command:
      - broker
    env_file:
      - ./druid/config/druid.env
    networks:
      - etl-net

  druid-historical:
    image: apache/druid:30.0.0
    container_name: druid-historical
    volumes:
      - druid_shared:/opt/shared
      - historical_var:/opt/druid/var
    depends_on:
      - zookeeper
      - postgres
      - druid-coordinator
    ports:
      - "8083:8083"
    command:
      - historical
    env_file:
      - ./druid/config/druid.env
    networks:
      - etl-net

  druid-middlemanager:
    image: apache/druid:30.0.0
    container_name: druid-middlemanager
    volumes:
      - druid_shared:/opt/shared
      - middlemanager_var:/opt/druid/var
    depends_on:
      - zookeeper
      - postgres
      - druid-coordinator
    ports:
      - "8091:8091"
      - "8100-8105:8100-8105"
    command:
      - middleManager
    env_file:
      - ./druid/config/druid.env
    networks:
      - etl-net

  druid-router:
    image: apache/druid:30.0.0
    container_name: druid-router
    volumes:
      - router_var:/opt/druid/var
    depends_on:
      - zookeeper
      - postgres
      - druid-coordinator
    ports:
      - "8888:8888"
    command:
      - router
    env_file:
      - ./druid/config/druid.env
    networks:
      - etl-net

  ######################################################
  # HADOOP SERVICES
  ######################################################
  # hadoop-base:
  #   build:
  #     context: ./hadoop/
  #     dockerfile: Dockerfile
  #   image: hadoop-base:latest
  #   container_name: hadoop-base
  #   environment:
  #     - HADOOP_VERSION=3.3.4
  #   command: [ "echo", "Build helper finished!" ]

  # namenode:
  #   image: hadoop-base:latest
  #   # restart: on-failure
  #   container_name: namenode
  #   depends_on:
  #     - hadoop-base
  #   ports:
  #     - "9870:9870"
  #   volumes:
  #     - ./hadoop/script/start-namenode.sh:/start-namenode.sh
  #     - hadoop_namenode:/hadoop/dfs/name
  #   networks:
  #     - etl-net
  #   environment:
  #     - CLUSTER_NAME=hadoop_cluster
  #     - HDFS_CONF_DFS_NAMENODE_NAME_DIR=file:///hadoop/dfs/name
  #   command: >
  #     /bin/bash -c " mkdir -p /hadoop/dfs/name && chmod +x /start-namenode.sh && ./start-namenode.sh "
  #   entrypoint: [ "/bin/bash", "-c" ]

  # datanode:
  #   image: hadoop-base:latest
  #   # restart: on-failure
  #   container_name: datanode
  #   depends_on:
  #     - namenode
  #     - hadoop-base
  #   ports:
  #     - "9864:9864"
  #   volumes:
  #     - ./hadoop/script/start-datanode.sh:/start-datanode.sh
  #     - hadoop_datanode:/hadoop/dfs/data
  #   networks:
  #     - etl-net
  #   environment:
  #     - SERVICE_PRECONDITION=namenode:9870
  #     - HDFS_CONF_DFS_DATANODE_DATA_DIR=file:///hadoop/dfs/data
  #   command: >
  #     /bin/bash -c " mkdir -p /hadoop/dfs/data && chmod +x /start-datanode.sh && ./start-datanode.sh "
  #   entrypoint: [ "/bin/bash", "-c" ]

  # resourcemanager:
  #   image: hadoop-base:latest
  #   # restart: on-failure
  #   container_name: resourcemanager
  #   depends_on:
  #     - namenode
  #     - hadoop-base
  #   ports:
  #     - "8088:8088"
  #   volumes:
  #     - ./hadoop/script/start-resourcemanager.sh:/start-resourcemanager.sh
  #     - hadoop_resourcemanager:/hadoop/dfs/data
  #   networks:
  #     - etl-net
  #   command: >
  #     /bin/bash -c " chmod +x /start-resourcemanager.sh && ./start-resourcemanager.sh "
  #   entrypoint: [ "/bin/bash", "-c" ]

######################################################
# NETWORK DEFINITION
######################################################

networks:
  etl-net:
    name: etl
    driver: bridge
    ipam:
      config:
        - subnet: 10.5.0.0/16
          gateway: "10.5.0.1"

######################################################
# VOLUMES DEFINITION
######################################################
volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  minio_data:
    driver: local
  minio_certs:
    driver: local
  druid_shared:
    driver: local
  coordinator_var:
    driver: local
  broker_var:
    driver: local
  historical_var:
    driver: local
  middlemanager_var:
    driver: local
  overlord_var:
    driver: local
  router_var:
    driver: local
  hadoop_namenode:
    driver: local
  hadoop_datanode:
    driver: local
  hadoop_resourcemanager:
    driver: local
  hadoop_historyserver:
    driver: local
